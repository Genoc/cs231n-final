{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull a small set of data and overfit on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 12)\n",
      "(40, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image, ImageFilter\n",
    "import random\n",
    "import cv2\n",
    "import os, glob\n",
    "\n",
    "t = pd.read_csv('../data/all_data_info.csv'); t.head()\n",
    "\n",
    "# filter down to just the first set of training data\n",
    "train_1 = t[t['new_filename'].str.startswith('1')]\n",
    "train_1 = train_1[train_1['in_train']]\n",
    "\n",
    "# print(train_1.shape)\n",
    "\n",
    "# pull a random sample of 200 impressionsim and 200 realism paintings\n",
    "df1 = train_1[train_1['style'].str.startswith('Impre', na=False)].sample(n=200)\n",
    "df2 = train_1[train_1['style'].str.startswith('Reali', na=False)].sample(n=200)\n",
    "\n",
    "# split data into train and val\n",
    "train_df1 = df1.sample(n=180)\n",
    "train_df2 = df2.sample(n=180)\n",
    "\n",
    "val_df1 = df1.loc[~df1.index.isin(train_df1.index)]\n",
    "val_df2 = df2.loc[~df2.index.isin(train_df2.index)]\n",
    "\n",
    "# print(val_df1.shape)\n",
    "# print(train_df1.shape)\n",
    "\n",
    "train_df = pd.concat([train_df1, train_df2])\n",
    "val_df = pd.concat([val_df1, val_df2])\n",
    "\n",
    "# print(train_df.shape)\n",
    "# print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load in imags with cv2, then convert to matrices that work with pytorch\n",
    "\n",
    "for i in train_df1['new_filename']:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# Return dataloaders for train, test, and val\n",
    "def load_images(train_df, test_df, val_df):\n",
    "    # load the image files and store as 3-channel tensors\n",
    "    \n",
    "    # create the dataloaders\n",
    "    train = data_utils.TensorDataset(X, y)\n",
    "    loader_train = DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "    test = data_utils.TensorDataset(X, y)\n",
    "    loader_test = DataLoader(test, batch_size=64, shuffle=True)\n",
    "    \n",
    "    val = data_utils.TensorDataset(X, y)\n",
    "    loader_val = DataLoader(val, batch_size=64, shuffle=True)    \n",
    "    \n",
    "    return [loader_train, load_test, loader_val]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "print_every = 100\n",
    "\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def train(model, loss_fn, optimizer, loader_train, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "            scores = model(x_var)\n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype), volatile=True)\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data here using load_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a simple network on our simplified dataset - should overfit it\n",
    "\n",
    "simple_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, stride=1),\n",
    "    nn.Conv2d(3, 32, kernel_size=3, stride=2), # -> 110\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d(2), # -> 55\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=2), # -> 27\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d(2), # -> 13\n",
    "    Flatten(),\n",
    "    nn.Linear(5408, 100),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(100, 2),\n",
    ")\n",
    "\n",
    "simple_model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimize = optim.Adam(simple_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we're going to feed a random batch into the model and make sure the output is the right size\n",
    "x = torch.randn(64, 3, 224, 224).type(dtype)\n",
    "x_var = Variable(x.type(dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model(x_var)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overfit a resnet via transfer learning on this small set of data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
